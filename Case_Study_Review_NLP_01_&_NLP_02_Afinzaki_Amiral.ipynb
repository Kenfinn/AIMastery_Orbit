{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Case Study: Review NLP 01 & NLP 02 Afinzaki Amiral.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kenfinn/AIMastery_Orbit/blob/main/Case_Study_Review_NLP_01_%26_NLP_02_Afinzaki_Amiral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: SMS Spam Classification\n",
        "Created by NLP Squad - Orbit Future Academy<br>\n",
        "Senin, 28 Maret 2022\n",
        "\n",
        "\n",
        "![title](https://undangmi.com/wp-content/uploads/2022/03/Screen-Shot-2022-03-26-at-23.14.46.png)\n",
        "\n",
        "Langkah-langkah yang diperlukan:\n",
        "1. Akuisisi data\n",
        "2. Text Preprocessing\n",
        "  1. Case folding\n",
        "  2. Word Normalization\n",
        "  3. Stopword removal\n",
        "  4. Stemming\n",
        "3. Feature Engineering\n",
        "  1. Feature Extraction - Bag of Words\n",
        "  2. Feature Extraction - TFIDF\n",
        "  3. Feature Selection - Chi Square\n",
        "4. Modelling (Machine Learning)# coming soon\n",
        "5. Model Evaluation # coming soon\n",
        "6. Deployment # coming soon"
      ],
      "metadata": {
        "id": "N9gInvQAL8-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "5QF1OxobMpfj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ8TNp9XG6Qr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sastrawi"
      ],
      "metadata": {
        "id": "Xr_2l-oFM3Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Lu3oGcvBNOZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk mempermudah kita dalam menyimpan objek agar dapat kita gunakan untuk pemodelan maupun deployment\n",
        "import pickle"
      ],
      "metadata": {
        "id": "DK_NS2hcNaMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat folder pada Google Drive untuk eksperimen\n",
        "# Sambungkan Google Colab dengan Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "tnEIZYoQvv66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pindahkan direktori ke folder eksperimen\n",
        "%cd /content/gdrive/MyDrive/Colab Notebooks/Group 2"
      ],
      "metadata": {
        "id": "J9fEbSzAwIkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "tMwge0-Zxa6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Acquisition\n",
        "Penjelasan label:\n",
        "- 0 --> SMS Normal\n",
        "- 1 --> SMS Fraud atau penipuan\n",
        "- 2 --> SMS Promo"
      ],
      "metadata": {
        "id": "4B7tNeNiPGXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/dataset_sms_spam_v1.csv"
      ],
      "metadata": {
        "id": "sO_KWJpRPPhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('dataset_sms_spam_v1.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "iIZJgqt8PT2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "otlzwkLDPcTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total Jumlah SMS:', data.shape[0], 'data\\n')\n",
        "print('terdiri dari (label):')\n",
        "print('-- [0] SMS Normal\\t:', data[data.label == 0].shape[0], 'data')\n",
        "print('-- [1] Fraud / Penipuan\\t:', data[data.label == 1].shape[0], 'data')\n",
        "print('-- [2] Promo\\t\\t:', data[data.label == 2].shape[0], 'data\\n')"
      ],
      "metadata": {
        "id": "XlwNpVS4RKPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "height = data['label'].value_counts()\n",
        "labels = ('SMS Normal', 'SMS Fraud / Penipuan', 'SMS Promo')\n",
        "y_pos = np.arange(len(labels))\n",
        "\n",
        "plt.figure(figsize=(7,4), dpi=80)\n",
        "plt.ylim(0,600)\n",
        "plt.title('Distribusi Kategori SMS', fontweight='bold')\n",
        "plt.xlabel('Kategori', fontweight='bold')\n",
        "plt.ylabel('Jumlah', fontweight='bold')\n",
        "plt.bar(y_pos, height, color=['deepskyblue', 'royalblue', 'skyblue'])\n",
        "plt.xticks(y_pos, labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qbXn2hhbR3C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "QITX3LRwSV9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case Folding"
      ],
      "metadata": {
        "id": "RTZpq2LmSgU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Buat fungsi untuk melakukan case folding\n",
        "def casefolding(text):\n",
        "  text = text.lower()                                 # Ubah jadi lowercase\n",
        "  text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)   # Menghapus URL\n",
        "  text = re.sub(r'[-+]?[0-9]+', '', text)             # Menghapus karakter angka\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)                 # Menghapus karakter tanda baca\n",
        "  text = text.strip()\n",
        "  return text"
      ],
      "metadata": {
        "id": "fn-tLwYISYL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = data['teks'].iloc[5]\n",
        "case_folding = casefolding(raw_text)\n",
        "\n",
        "print('Raw text\\t: ', raw_text)\n",
        "print('Case folding\\t: ', case_folding)"
      ],
      "metadata": {
        "id": "YOkItJ_nTovJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Normalization"
      ],
      "metadata": {
        "id": "PvAaG9QJW5U_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download corpus singkatan\n",
        "!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/key_norm.csv"
      ],
      "metadata": {
        "id": "FAOZNVKxT6O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_norm = pd.read_csv('key_norm.csv')\n",
        "\n",
        "# Buat fungsi untuk melakukan word normalization\n",
        "def text_normalize(text):\n",
        "  text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])\n",
        "  text = str.lower(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "ZkQMRVv2W9Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = data['teks'].iloc[5]\n",
        "case_folding = casefolding(raw_text)\n",
        "word_normalization = text_normalize(case_folding)\n",
        "\n",
        "print('Raw text\\t: ', raw_text)\n",
        "print('Case folding\\t: ', case_folding)\n",
        "print('Word normalization\\t: ', word_normalization)"
      ],
      "metadata": {
        "id": "zgOmLRd5Xnuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering (Stopwords Removal)"
      ],
      "metadata": {
        "id": "PuCl9K8xZQ5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_id = stopwords.words('indonesian')"
      ],
      "metadata": {
        "id": "UlngQHeTZQZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords_id)"
      ],
      "metadata": {
        "id": "oxeuN0I3Xv3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lihat daftar stopwords Indonesia yang disediakan NLTK\n",
        "stopwords_id"
      ],
      "metadata": {
        "id": "Hkj_DOCmZkw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat fungsi untuk langkah stopwords removal\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  clean_word = []\n",
        "  all_text = text.split()\n",
        "  for word in all_text:\n",
        "    if word not in stopwords_id:\n",
        "      clean_word.append(word)\n",
        "  return ' '.join(clean_word)"
      ],
      "metadata": {
        "id": "yHcQp6rNaE-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = data['teks'].iloc[5]\n",
        "case_folding = casefolding(raw_text)\n",
        "word_normalization = text_normalize(case_folding)\n",
        "stopwords_removal = remove_stopwords(word_normalization)\n",
        "\n",
        "print('Raw text\\t: ', raw_text)\n",
        "print('Case folding\\t: ', case_folding)\n",
        "print('Word normalization\\t: ', word_normalization)\n",
        "print('Stopwords removal\\t: ', stopwords_removal)"
      ],
      "metadata": {
        "id": "ou0bBECVa8zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "bsPP1l2BbpQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Buat fungsi untuk langkah stemming Bahasa Indonesia\n",
        "def stemming(text):\n",
        "  text = stemmer.stem(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "PnJHItZZbPjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = data['teks'].iloc[5]\n",
        "case_folding = casefolding(raw_text)\n",
        "word_normalization = text_normalize(case_folding)\n",
        "stopwords_removal = remove_stopwords(word_normalization)\n",
        "text_stemming = stemming(stopwords_removal)\n",
        "\n",
        "print('Raw text\\t: ', raw_text)\n",
        "print('Case folding\\t: ', case_folding)\n",
        "print('Word normalization\\t: ', word_normalization)\n",
        "print('Stopwords removal\\t: ', stopwords_removal)\n",
        "print('Stemming\\t: ', text_stemming)"
      ],
      "metadata": {
        "id": "wHPus3cSb3HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing Pipeline\n"
      ],
      "metadata": {
        "id": "25ms8w-pdO5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat fungsi untuk menggabungkan seluruh langkah pada text preprocessing\n",
        "def text_preprocessing_process(text):\n",
        "  text = casefolding(text)\n",
        "  text = text_normalize(text)\n",
        "  text = remove_stopwords(text)\n",
        "  text = stemming(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "qDicj7iQb_7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "data['clean_teks'] = data['teks'].apply(text_preprocessing_process)\n",
        "\n",
        "# Perhatikan waktu komputasi ketika melakukan text preprocessing"
      ],
      "metadata": {
        "id": "wM74wRf-dz2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "WfPX3JMkeFJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan hasil data yang telah melalui proses text preprocessing\n",
        "data.to_csv('clean_data.csv')"
      ],
      "metadata": {
        "id": "OJN0Pvdtfpkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "Kb3GxAC3hg1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pisahkan kolom feature dan target\n",
        "X = data['clean_teks']\n",
        "y = data['label']"
      ],
      "metadata": {
        "id": "zPZglr0-f3Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "wgAD7DHCn8Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "-Y0W_1kjn9KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction (Bag of Words & N-Gram)\n",
        "Proses mengubah teks menjadi bentuk vektor menggunakan metode BoW"
      ],
      "metadata": {
        "id": "lrUMAvf1oQxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Convert a collection of text documents to a matrix of token counts.\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "'''\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Bag of Words menggunakan unigram\n",
        "bow = CountVectorizer(ngram_range=(1,1))\n",
        "bow.fit(X)"
      ],
      "metadata": {
        "id": "jbbGznB8oAga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat jumlah token / fitur yang dihasilkan Bag of Words\n",
        "len(bow.get_feature_names_out())"
      ],
      "metadata": {
        "id": "FVaOqD3Ao2LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat fitur-fitur apa saja yang ada dalam korpus\n",
        "bow.get_feature_names_out()"
      ],
      "metadata": {
        "id": "OgrjDxMsqw_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat matriks jumlah token\n",
        "# Hasil ini, siap dimasukkan (di fit kan) dalam proses modelling (machine learning)\n",
        "\n",
        "X_bow = bow.transform(X).toarray()\n",
        "X_bow"
      ],
      "metadata": {
        "id": "DrvUWdNosUCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_bow = pd.DataFrame(X_bow, columns=bow.get_feature_names_out())\n",
        "data_bow"
      ],
      "metadata": {
        "id": "FgwWkn6VsmGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan objek dari hasil bag of words\n",
        "with open('hasil_bow.pickle', 'wb') as ouput:\n",
        "  pickle.dump(X_bow, ouput)"
      ],
      "metadata": {
        "id": "lgvs90MOtZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction (TF-IDF & N-Gram)\n",
        "Proses mengubah teks menjadi bentuk vektor menggunakan metode TF-IDF"
      ],
      "metadata": {
        "id": "Ii9B-rZiyMCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "'''\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf_idf = TfidfVectorizer(ngram_range=(1,1))\n",
        "tf_idf.fit(X)\n",
        "\n",
        "X_tf_idf = tf_idf.transform(X)"
      ],
      "metadata": {
        "id": "Z9hbho4Lunzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat jumlah token / fitur yang dihasilkan TF-IDF\n",
        "len(tf_idf.get_feature_names_out())"
      ],
      "metadata": {
        "id": "5do2ZVYay3tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat fitur-fitur apa saja yang ada dalam korpus\n",
        "tf_idf.get_feature_names_out()"
      ],
      "metadata": {
        "id": "bP_xTW8fzApA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat matriks token menggunakan TF-IDF, lihat perbandingannya dengan metode BoW\n",
        "# Sama halnya dengan BoW, data ini siap dimasukkan ke proses modelling\n",
        "\n",
        "X_tf_idf = tf_idf.transform(X).toarray()\n",
        "X_tf_idf"
      ],
      "metadata": {
        "id": "P4LymJNBzMU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_tf_idf = pd.DataFrame(X_tf_idf, columns=tf_idf.get_feature_names_out())\n",
        "data_tf_idf"
      ],
      "metadata": {
        "id": "3bwZ1cy3zdlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan objek dari hasil TF-IDF\n",
        "with open('hasil_tf_idf.pickle', 'wb') as ouput:\n",
        "  pickle.dump(X_tf_idf, ouput)"
      ],
      "metadata": {
        "id": "fSIA9Eq40AmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "x0mNgCJU0PLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah nilai data tabular tf-idf menjadi array agar dapat dijalankan pada proses seleksi fitur\n",
        "X = np.array(data_tf_idf)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "LLRdBhNx0K5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Select features according to the k highest scores.\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
        "\n",
        "Compute chi-squared stats between each non-negative feature and class.\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html\n",
        "'''\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest \n",
        "from sklearn.feature_selection import chi2 \n",
        "\n",
        "# K features with highest chi-squared statistics are selected \n",
        "chi2_features = SelectKBest(chi2, k=1000) \n",
        "X_kbest_features = chi2_features.fit_transform(X, y) \n",
        "  \n",
        "# Reduced features \n",
        "print('Original feature number:', X.shape[1]) \n",
        "print('Reduced feature number:', X_kbest_features.shape[1]) "
      ],
      "metadata": {
        "id": "Sf761Jt60aBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chi2_features.scores_ adalah nilai chi-square, semakin tinggi nilainya maka semakin baik fiturnya\n",
        "data_chi2 = pd.DataFrame(chi2_features.scores_, columns=['nilai'])\n",
        "data_chi2"
      ],
      "metadata": {
        "id": "EMxBMzdM04T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan fitur beserta nilainya\n",
        "feature = tf_idf.get_feature_names_out()\n",
        "data_chi2['fitur'] = feature\n",
        "data_chi2"
      ],
      "metadata": {
        "id": "WL-eUjkT0_Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengurutkan fitur terbaik\n",
        "data_chi2.sort_values(by='nilai', ascending=False)"
      ],
      "metadata": {
        "id": "6dt8ufYq1FV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan mask pada feature yang diseleksi\n",
        "# False berarti fitur tidak terpilih dan True berarti fitur terpilih\n",
        "mask = chi2_features.get_support()\n",
        "mask"
      ],
      "metadata": {
        "id": "a_2Grgsr1Td2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan fitur-fitur terpilih berdasarkan mask atau nilai tertinggi yang sudah dikalkulasi pada Chi-Square\n",
        "new_feature = []\n",
        "for bool, f in zip(mask, feature):\n",
        "  if bool:\n",
        "    new_feature.append(f)\n",
        "  selected_feature = new_feature\n",
        "selected_feature"
      ],
      "metadata": {
        "id": "pHJcwGb31Xfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan fitur-fitur yang sudah diseleksi \n",
        "# Beserta nilai vektornya pada keseluruhan data untuk dijalankan pada proses machine learning\n",
        "\n",
        "# Hanya k fitur yang terpilih sesuai parameter k yang ditentukan sebelumnya\n",
        "\n",
        "data_selected_feature = pd.DataFrame(X_kbest_features, columns=selected_feature)\n",
        "data_selected_feature"
      ],
      "metadata": {
        "id": "41r4rW4L1gIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('best_feature_chi2.pickle', 'wb') as output:\n",
        "  pickle.dump(X_kbest_features, output)"
      ],
      "metadata": {
        "id": "R64y5vJe1rLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling (Machine Learning)"
      ],
      "metadata": {
        "id": "27Kl90_N15cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coming Soon"
      ],
      "metadata": {
        "id": "7Y9f3CuN16-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "TrVgTrk218i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coming Soon"
      ],
      "metadata": {
        "id": "G3BfGuTv1-LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "vktdz1QE2AAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coming Soon"
      ],
      "metadata": {
        "id": "5CJpusA72Bn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordCloud"
      ],
      "metadata": {
        "id": "vGJV4Rvq2T0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library WordCloud. WordCloud digunakan untuk melihat secara visual kata-kata yang paling sering muncul.\n",
        "# Import Library cv2 untuk mengolah gambar menjadi masking WordCloud\n",
        "\n",
        "import cv2\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "hPdL39RV2VsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download gambar masking\n",
        "!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/img/cloud.jpg"
      ],
      "metadata": {
        "id": "ULlmkEa72YTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "originalImage = cv2.imread('cloud.jpg')\n",
        "grayImage = cv2.cvtColor(originalImage, cv2.COLOR_BGR2GRAY)\n",
        "(thresh, cloud_mask) = cv2.threshold(grayImage, 100, 255, cv2.THRESH_BINARY)"
      ],
      "metadata": {
        "id": "4s-Hsoam2air"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan masking\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "cv2_imshow(cloud_mask)"
      ],
      "metadata": {
        "id": "z8R3CQ8G2h_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud Label SMS Normal\n",
        "\n",
        "sms_normal = data[data.label == 0]\n",
        "normal_string = []\n",
        "\n",
        "for t in sms_normal.clean_teks:\n",
        "  normal_string.append(t)\n",
        "\n",
        "normal_string = pd.Series(normal_string).str.cat(sep=' ')\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wordcloud = WordCloud(width=1600, height=800, margin=10,\n",
        "                      background_color='white', colormap='Dark2',\n",
        "                      max_font_size=200, min_font_size=25,\n",
        "                      mask=cloud_mask, contour_width=10, contour_color='firebrick',\n",
        "                      max_words=100).generate(normal_string)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IajM8Q9Q2qlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud Label SMS Fraud / Penipuan\n",
        "\n",
        "sms_fraud = data[data.label == 1]\n",
        "fraud_string = []\n",
        "\n",
        "for t in sms_fraud.clean_teks:\n",
        "  fraud_string.append(t)\n",
        "\n",
        "fraud_string = pd.Series(fraud_string).str.cat(sep=' ')\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wordcloud = WordCloud(width=1600, height=800, margin=10,\n",
        "                      background_color='white', colormap='Dark2',\n",
        "                      max_font_size=200, min_font_size=25,\n",
        "                      mask=cloud_mask, contour_width=10, contour_color='firebrick',\n",
        "                      max_words=100).generate(fraud_string)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PGAncS7-244A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud Label SMS Promo\n",
        "\n",
        "sms_promo = data[data.label == 2]\n",
        "promo_string = []\n",
        "\n",
        "for t in sms_promo.clean_teks:\n",
        "  promo_string.append(t)\n",
        "\n",
        "promo_string = pd.Series(promo_string).str.cat(sep=' ')\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wordcloud = WordCloud(width=1600, height=800, margin=10,\n",
        "                      background_color='white', colormap='Dark2',\n",
        "                      max_font_size=200, min_font_size=25,\n",
        "                      mask=cloud_mask, contour_width=10, contour_color='firebrick',\n",
        "                      max_words=100).generate(promo_string)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mwpr_61u3AOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Activity\n",
        "Lakukan proses di atas menggunakan dataset review product https://drive.google.com/file/d/1qn5WXp-H95_FL_Rx5oqvfZaflYdHsnrF/view?usp=sharing\n",
        "\n",
        "Tugas Anda:\n",
        "- Tentukan langkah text pre-processing yang tepat untuk dataset di atas.\n",
        "- Gunakan range `n_gram` yang berbeda. Amati apa perbedaannya.\n",
        "- Menurut Anda, apakah `term` yang dihasilkan (`X_kbest_features`) pada feature selection sudah memiliki informasi yang relevan?\n",
        "\n",
        "Setelah dikerjakan, buatlah resume berdasarkan pengalaman Anda dalam melakukan text pre-processing dan feature engineering.\n",
        "\n",
        "Kumpulkan tugas Anda pada: https://s.id/tugas-nlp-ofa\n",
        "\n",
        "Batas pengumpulan maksimal: Jumat, 1 April 2022"
      ],
      "metadata": {
        "id": "0M4TQARq3UBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "szbBz91K34vQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}